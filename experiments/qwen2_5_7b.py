# -*- coding: utf-8 -*-
"""Qwen2.5-7B.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CNttBW3ez7OG8vPfLSsFylbPRuwPVbR6
"""

!pip install datasets

import fsspec
import time
import json
import csv
import psutil
import torch
import os

import matplotlib.pyplot as plt

from aiohttp import ClientTimeout
from datasets import load_dataset, DownloadConfig
# Resource limit modules often have partial support on macOS
from resource import setrlimit, getrlimit, RLIMIT_AS

from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers import pipeline

# Load deepmind/math_dataset (subset: algebra) from HF Hub
dataset_subset_size = 1
print("Loading deepmind/math_dataset (subset: algebra) ...")
storage_options = {
    "client_kwargs": {
        "timeout": ClientTimeout(total=12000)
    }
}
math_dataset = load_dataset("math_dataset", "algebra__linear_1d",
                            split="train",
                            storage_options=storage_options)
math_dataset = math_dataset.select(range(dataset_subset_size))

THREAD_CONFIGS = [1, 2]
RAM_CONFIGS = [None, 4096]

def evaluate_answer(generated_answer, reference_solution):

    if not reference_solution:
        return False


    import re
    ref_nums = re.findall(r"[+-]?\d+(?:\.\d+)?", reference_solution)
    gen_nums = re.findall(r"[+-]?\d+(?:\.\d+)?", generated_answer)


    if len(ref_nums) == 1:
        return ref_nums[0] in gen_nums


    return reference_solution.strip() in generated_answer

model_name = "Qwen/Qwen2.5-7B"
num_threads = 2
max_ram_mb = None
output_file = "cpu_ram_limit_results.json"
plot_results = True

torch.set_num_threads(num_threads)

if max_ram_mb is not None:
    soft, hard = getrlimit(RLIMIT_AS)
    try:
        setrlimit(RLIMIT_AS, (max_ram_mb * 1024 * 1024, hard))
        print(f"Attempted to set max RAM to {max_ram_mb} MB.")
    except ValueError as e:
        print(f"Could not set memory limit: {e}")

# Load model/tokenizer using the transformers library
print(f"Loading model {model_name} with {num_threads} threads ...")
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-7B", trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2.5-7B", trust_remote_code=True)
model.eval()
pipe = pipeline("text-generation", model="Qwen/Qwen2.5-7B", torch_dtype=torch.bfloat16, device_map="auto")

# We'll store results in a dictionary for JSON
results = {
    "model_name": model_name,
    "num_threads": num_threads,
    "max_ram_mb": max_ram_mb,
    "dataset_subset_size": dataset_subset_size,
    "inference_stats": []
}

# Benchmark inference
for i, example in enumerate(math_dataset):
    input_text = example["question"]
    if input_text.startswith("b'") and input_text.endswith("'"):
        input_text = input_text[2:-1]
    if isinstance(input_text, bytes):
        input_text = input_text.decode("utf-8")
    problem = input_text.strip().replace("\n", " ") + "\nSolution:"

    start_time = time.time()

    # Measure CPU and memory before inference
    cpu_usage_before = psutil.cpu_percent(interval=None)
    mem_usage_before = psutil.virtual_memory().used

    messages = [
    {
        "role": "system",
        "content": "You are a friendly chatbot who always responds in the style of a pirate",
    },
    {"role": "user", "content": problem},
    ]

    prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    modelResp = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)
    full_response = modelResp[0]["generated_text"]
    if "Solution:" in full_response:
        answer_part = full_response.split("Solution:", 1)[-1]
        answer_part = answer_part.split("Problem:", 1)[0]
        answer = answer_part.strip()
    else:
        # fallback if no "Solution:" found
        answer = full_response.strip()

    # Measure CPU and memory after inference
    cpu_usage_after = psutil.cpu_percent(interval=None)
    mem_usage_after = psutil.virtual_memory().used

    end_time = time.time()
    elapsed_time_s = end_time - start_time

    # Collect stats
    result_entry = {
        "example_index": i,
        "cpu_usage_before": cpu_usage_before,
        "cpu_usage_after": cpu_usage_after,
        "mem_usage_before": mem_usage_before,
        "mem_usage_after": mem_usage_after,
        "elapsed_time_s": elapsed_time_s,
        "input_text": input_text,
        "generated_text": full_response,
    }
    results["inference_stats"].append(result_entry)

def main():


    dataset_subset_size = 2
    model_name = "Qwen/Qwen2.5-7B"
    output_file = "cpu_ram_limit_results.json"
    plot_results = True
    output_dir = os.path.join("output", "logs")
    os.makedirs(output_dir, exist_ok=True)


    all_results = []
    for thread_val in THREAD_CONFIGS:
        for ram_val in RAM_CONFIGS:
            print("\n===========================")
            print(f"RUN EXPERIMENT: threads={thread_val}, max_ram={ram_val} MB")
            print("===========================\n")


            exp_results = run_experiment(
                model_name=model_name,
                num_threads=thread_val,
                max_ram_mb=ram_val,
                dataset=math_dataset,
                output_dir=output_dir,
                dataset_subset_size=dataset_subset_size
            )
            all_results.extend(exp_results)


    output_path_json = os.path.join(output_dir, output_file)
    with open(output_path_json, "w", encoding="utf-8") as f:
        json.dump(all_results, f, indent=2)
    print(f"All experiments finished. JSON results saved to {output_path_json}")


    if output_file.endswith(".json"):
        csv_file = output_file.replace(".json", ".csv")
    else:
        csv_file = output_file + ".csv"
    output_path_csv = os.path.join(output_dir, csv_file)

    csv_headers = list(all_results[0].keys()) if all_results else []
    with open(output_path_csv, mode="w", newline="", encoding="utf-8") as csv_f:
        writer = csv.DictWriter(csv_f, fieldnames=csv_headers)
        writer.writeheader()
        for stat in all_results:
            writer.writerow(stat)

    print(f"CSV results also saved to {output_path_csv}")


    if plot_results:
        print("Plotting results ...")
        import pandas as pd
        df = pd.read_csv(output_path_csv)


        plt.figure()

        groups = df.groupby(["num_threads", "max_ram_mb"])
        for (t_val, r_val), group_data in groups:

            group_data = group_data.sort_values("example_index")
            label_str = f"threads={t_val}, ram={r_val}"
            plt.plot(
                group_data["example_index"],
                group_data["elapsed_time_s"],
                marker="o",
                label=label_str
            )
        plt.title("Inference Time vs Example Index (Various Threads & RAM)")
        plt.xlabel("Example Index")
        plt.ylabel("Elapsed Time (s)")
        plt.legend()
        plt.savefig(os.path.join(output_dir, "inference_time_compare.png"))
        plt.close()

        acc_summary = groups["is_correct"].mean().reset_index(name="accuracy")

        acc_summary["accuracy"] = acc_summary["accuracy"] * 100.0

        plt.figure()

        x_labels = [f"(T={row['num_threads']},RAM={row['max_ram_mb']})" for _, row in acc_summary.iterrows()]
        plt.bar(x_labels, acc_summary["accuracy"])
        plt.title("Accuracy Comparison (Threads x RAM)")
        plt.xlabel("(Threads, RAM)")
        plt.ylabel("Accuracy (%)")
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, "accuracy_compare.png"))
        plt.close()

        print("Plots saved to:", output_dir)

output_dir = os.path.join("output", "logs")
os.makedirs(output_dir, exist_ok=True)

# Save results to JSON
output_path_json = os.path.join(output_dir, output_file)
with open(output_path_json, "w") as f:
    json.dump(results, f, indent=2)
print(f"Experiment finished. JSON results saved to {output_path_json}")

# Save results to CSV
if output_file.endswith(".json"):
    csv_file = output_file.replace(".json", ".csv")
else:
    csv_file = output_file + ".csv"
output_path_csv = os.path.join(output_dir, csv_file)

csv_headers = [
    "example_index",
    "cpu_usage_before",
    "cpu_usage_after",
    "mem_usage_before",
    "mem_usage_after",
    "elapsed_time_s",
    "input_text",
    "generated_text"
]
with open(output_path_csv, mode="w", newline="", encoding="utf-8") as csv_f:
    writer = csv.DictWriter(csv_f, fieldnames=csv_headers)
    writer.writeheader()
    for stat in results["inference_stats"]:
        writer.writerow(stat)

print(f"CSV results also saved to {output_path_csv}")

if plot_results:
    print("Plotting results ...")
    example_indices = []
    cpu_before_vals = []
    cpu_after_vals = []
    mem_before_vals = []
    mem_after_vals = []
    elapsed_times = []

    with open(output_path_csv, "r", encoding="utf-8") as csv_in:
        reader = csv.DictReader(csv_in)
        for row in reader:
            ex_id = int(row["example_index"])
            cpu_b = float(row["cpu_usage_before"])
            cpu_a = float(row["cpu_usage_after"])
            mem_b = float(row["mem_usage_before"])
            mem_a = float(row["mem_usage_after"])
            e_time = float(row["elapsed_time_s"])

            example_indices.append(ex_id)
            cpu_before_vals.append(cpu_b)
            cpu_after_vals.append(cpu_a)
            mem_before_vals.append(mem_b)
            mem_after_vals.append(mem_a)
            elapsed_times.append(e_time)

    # Plot 1: Inference time vs. example index
    plt.figure()
    plt.plot(example_indices, elapsed_times, marker="o")
    plt.title("Inference Time vs Example Index")
    plt.xlabel("Example Index")
    plt.ylabel("Elapsed Time (s)")
    plt.savefig(os.path.join(output_dir, "inference_time_vs_index.png"))
    plt.close()

    # Plot 2: CPU usage before/after
    plt.figure()
    plt.plot(example_indices, cpu_before_vals, marker="o", label="CPU Before (%)")
    plt.plot(example_indices, cpu_after_vals, marker="x", label="CPU After (%)")
    plt.title("CPU Usage Before vs After Inference")
    plt.xlabel("Example Index")
    plt.ylabel("CPU Usage (%)")
    plt.legend()
    plt.savefig(os.path.join(output_dir, "cpu_usage.png"))
    plt.close()

    # Plot 3: Memory usage before/after
    plt.figure()
    plt.plot(example_indices, mem_before_vals, marker="o", label="Mem Before (bytes)")
    plt.plot(example_indices, mem_after_vals, marker="x", label="Mem After (bytes)")
    plt.title("Memory Usage Before vs After Inference")
    plt.xlabel("Example Index")
    plt.ylabel("Memory (bytes)")
    plt.legend()
    plt.savefig(os.path.join(output_dir, "mem_usage.png"))
    plt.close()

    print("Plots saved to:", output_dir)

main()

