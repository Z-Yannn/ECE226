# -*- coding: utf-8 -*-
"""gpt_quantized.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uUZhy0rjerUNtlcDVgbhimyberrvPbQO
"""

!pip install datasets

import fsspec
import time
import json
import csv
import psutil
import torch
import os
import matplotlib.pyplot as plt

from aiohttp import ClientTimeout
from datasets import load_dataset, DownloadConfig
from resource import setrlimit, getrlimit, RLIMIT_AS

from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers import pipeline

MODEL_VARIATIONS = [
    {"name": "openai-community/gpt2", "variation": "base"},
    {"name": "openai-community/gpt2-medium", "variation": "base"},
    {"name": "openai-community/gpt2", "variation": "quantized"},
    {"name": "openai-community/gpt2", "variation": "pruned"},
]

DEFAULT_MODEL_NAME = "openai-community/gpt2"

THREAD_CONFIGS = [1,2,4,8]
RAM_CONFIGS = [None]

DATASET_SUBSET_SIZE = 3
OUTPUT_FILE = "cpu_ram_limit_results_c.json"
PLOT_RESULTS = True
OUTPUT_DIR = os.path.join("output", "logs_c")
os.makedirs(OUTPUT_DIR, exist_ok=True)

EXPERIMENT_MODE = "threads"
def evaluate_answer(generated_answer, reference_solution):
    """
    一个非常简单的评估函数，示例做法：
    - 如果 reference_solution 里只有一个数字，并且 generated_answer 含有相同的数字，则判定正确
    - 或者直接做一个字符串包含的判断
    对 MATH 数据集的正式评测通常要做更复杂的解析，这里仅作 Demo。
    """
    if not reference_solution:
        return False
    import re
    ref_nums = re.findall(r"[+-]?\d+(?:\.\d+)?", reference_solution)
    gen_nums = re.findall(r"[+-]?\d+(?:\.\d+)?", generated_answer)

    if len(ref_nums) == 1:
        return ref_nums[0] in gen_nums

    return reference_solution.strip() in generated_answer

def run_inference_experiment(pipe, math_dataset, experiment_mode, thread_list, ram_list):
    all_results = []

    if experiment_mode == "threads":
        ram_val = ram_list[0] if ram_list else None
        for thread_val in thread_list:
            results = run_single_setting(pipe, math_dataset, thread_val, ram_val)
            all_results.extend(results)

    elif experiment_mode == "memory":
        thread_val = thread_list[0] if thread_list else 1
        for ram_val in ram_list:
            results = run_single_setting(pipe, math_dataset, thread_val, ram_val)
            all_results.extend(results)

    else:
        raise ValueError(f"Unknown experiment_mode: {experiment_mode}")

    return all_results

def run_single_setting(pipe, dataset, num_threads, max_ram_mb):
    print("\n===========================")
    print(f"RUN EXPERIMENT: threads={num_threads}, max_ram={max_ram_mb} MB")
    print("===========================\n")

    torch.set_num_threads(num_threads)

    if max_ram_mb is not None:
        max_ram_bytes = max_ram_mb * 1024 * 1024
        soft, hard = getrlimit(RLIMIT_AS)
        setrlimit(RLIMIT_AS, (max_ram_bytes, max_ram_bytes))
        print(f"Set memory limit to {max_ram_mb} MB (RLIMIT_AS).")
    else:
        print("No memory limit applied.")

    experiment_results = []
    for i, example in enumerate(dataset.select(range(DATASET_SUBSET_SIZE))):
        input_text = example["question"]
        reference_solution = example["answer"]
        if isinstance(input_text, bytes):
            input_text = input_text.decode("utf-8")
        if input_text.startswith("b'") and input_text.endswith("'"):
            input_text = input_text[2:-1]
        if reference_solution.startswith("b'") and reference_solution.endswith("'"):
            reference_solution = reference_solution[2:-3] # ground truth solution

        problem = input_text.strip().replace("\n", " ") + "\nSolution:"
        print(reference_solution)
        start_time = time.time()
        cpu_usage_before = psutil.cpu_percent(interval=None)
        mem_usage_before = psutil.virtual_memory().used

        messages = [
            {
                "role": "system",
                "content": "You are a friendly chatbot who always responds in the style of a pirate",
            },
            {
                "role": "user",
                "content": problem,
            },
        ]
        # prompt = pipe.tokenizer.apply_chat_template(
        #     messages, tokenize=False, add_generation_prompt=True
        # )
        model_resp = pipe(
            "text-generation",
            max_new_tokens=512,
            do_sample=True,
            temperature=0.7,
            top_k=50,
            top_p=0.95,
        )

        full_response = model_resp[0]["generated_text"]
        if "<|assistant|>" in full_response:
            answer = full_response.split("<|assistant|>", 1)[-1]

        else:
            answer = full_response.strip()

        cpu_usage_after = psutil.cpu_percent(interval=None)
        mem_usage_after = psutil.virtual_memory().used
        end_time = time.time()
        elapsed_time_s = end_time - start_time

        is_correct = evaluate_answer(answer, reference_solution)

        result_entry = {
            "model_name": pipe.model.__class__.__name__,  # pipe.model.config.name_or_path
            "num_threads": num_threads,
            "max_ram_mb": max_ram_mb,
            "example_index": i,
            "cpu_usage_before": cpu_usage_before,
            "cpu_usage_after": cpu_usage_after,
            "mem_usage_before": mem_usage_before,
            "mem_usage_after": mem_usage_after,
            "elapsed_time_s": elapsed_time_s,
            "is_correct": int(is_correct),
            "input_text": input_text,
            "reference_solution": reference_solution,
            "generated_text": answer,
        }

        experiment_results.append(result_entry)

    return experiment_results

print("Loading deepmind/math_dataset (subset: algebra__linear_1d) ...")
math_dataset = load_dataset("math_dataset", "algebra__linear_1d", split="train", trust_remote_code=True)

all_results = []
for var_cfg in MODEL_VARIATIONS:
    base_model_name = var_cfg["name"]
    model_variation = var_cfg["variation"]

    print(f"\nLoading model '{base_model_name}' with variation '{model_variation}' ...")
    tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(base_model_name, trust_remote_code=True)
    model.eval()

    if model_variation == "quantized":
        model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)
        print("Applied dynamic quantization.")

    if model_variation == "pruned":
        import torch.nn.utils.prune as prune
        for name, module in model.named_modules():
            if isinstance(module, torch.nn.Linear):
                prune.l1_unstructured(module, name="weight", amount=0.2)
        print("Applied pruning (20% L1 unstructured) to all linear layers.")

    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        torch_dtype=torch.bfloat16,
        device_map="cpu"
    )
    print("Model loaded and processed successfully.\n")

    print(f"Experiment Mode (hard-coded): {EXPERIMENT_MODE}")
    results = run_inference_experiment(
        pipe=pipe,
        math_dataset=math_dataset,
        experiment_mode=EXPERIMENT_MODE,
        thread_list=THREAD_CONFIGS,
        ram_list=RAM_CONFIGS
    )
    for r in results:
        r["model_variation"] = model_variation
        r["model_base"] = base_model_name
    all_results.extend(results)

output_path_json = os.path.join(OUTPUT_DIR, OUTPUT_FILE)
with open(output_path_json, "w", encoding="utf-8") as f:
    json.dump(all_results, f, indent=2)
print(f"All experiments finished. JSON results saved to {output_path_json}")

if OUTPUT_FILE.endswith(".json"):
    csv_file = OUTPUT_FILE.replace(".json", ".csv")
else:
    csv_file = OUTPUT_FILE + ".csv"
output_path_csv = os.path.join(OUTPUT_DIR, csv_file)

csv_headers = list(all_results[0].keys()) if all_results else []
with open(output_path_csv, mode="w", newline="", encoding="utf-8") as csv_f:
    writer = csv.DictWriter(csv_f, fieldnames=csv_headers)
    writer.writeheader()
    for stat in all_results:
        writer.writerow(stat)
print(f"CSV results also saved to {output_path_csv}")

import pandas as pd
print("Plotting results ...")
df = pd.read_csv(output_path_csv)
print(df.head())

df["num_threads"] = pd.to_numeric(df["num_threads"], errors='coerce')

grouped = df.groupby(["model_variation", "num_threads"], as_index=False).mean()
print("Grouped DataFrame:")
print(grouped)

plt.figure(figsize=(8,6))
for variation in grouped["model_variation"].unique():
    subset = grouped[grouped["model_variation"] == variation]
    plt.plot(subset["num_threads"], subset["elapsed_time_s"], marker='o', linestyle='-', label=variation)
plt.xlabel("Number of Threads")
plt.ylabel("Average Elapsed Time (s)")
plt.title("Average Inference Time vs Number of Threads\nfor Different Model Variations")
plt.legend(title="Model Variation")
plt.grid(True)
plt.xticks([1, 2, 4, 8])
plt.xlim(1, 8)
plot_path = os.path.join(OUTPUT_DIR, "inference_time_vs_threads_variations.png")
plt.savefig(plot_path)
plt.show()

print("Plots saved to:", OUTPUT_DIR)

print(all_results)

import pandas as pd
print("Plotting results ...")
df = pd.read_csv(output_path_csv)
print(df.head())
print(df)
df["num_threads"] = pd.to_numeric(df["num_threads"], errors='coerce')
df["elapsed_time_s"] = pd.to_numeric(df["elapsed_time_s"], errors='coerce')
df["is_correct"] = pd.to_numeric(df["is_correct"], errors='coerce')
df["cpu_usage_before"] = pd.to_numeric(df["cpu_usage_before"], errors='coerce')
df["cpu_usage_after"] = pd.to_numeric(df["cpu_usage_after"], errors='coerce')
df["mem_usage_before"] = pd.to_numeric(df["mem_usage_before"], errors='coerce')
df["mem_usage_after"] = pd.to_numeric(df["mem_usage_after"], errors='coerce')

df["cpu_diff"] = df["cpu_usage_after"] - df["cpu_usage_before"]
df["mem_diff"] = df["mem_usage_after"] - df["mem_usage_before"]

grouped = df.groupby(["model_variation", "num_threads"], as_index=False).mean(numeric_only=True)
print("Grouped DataFrame:")
print(grouped)
plt.figure(figsize=(8,6))
for variation in grouped["model_variation"].unique():
    subset = grouped[grouped["model_variation"] == variation]
    plt.plot(subset["num_threads"], subset["elapsed_time_s"], marker='o', linestyle='-', label=variation)
plt.xlabel("Number of Threads")
plt.ylabel("Average Elapsed Time (s)")
plt.title("Average Inference Time vs Number of Threads\nfor Different Model Variations")
plt.legend(title="Model Variation")
plt.grid(True)
plt.xticks([1, 2, 4, 8])
plt.xlim(1, 8)
plt.savefig(os.path.join(OUTPUT_DIR, "inference_time_vs_threads_variations.png"))
plt.show()

plt.figure(figsize=(8,6))
for variation in grouped["model_variation"].unique():
    subset = grouped[grouped["model_variation"] == variation]
    plt.plot(subset["num_threads"], subset["is_correct"], marker='o', linestyle='-', label=variation)
plt.xlabel("Number of Threads")
plt.ylabel("Average Correctness Rate")
plt.title("Average Correctness vs Number of Threads\nfor Different Model Variations")
plt.legend(title="Model Variation")
plt.grid(True)
plt.xticks([1, 2, 4, 8])
plt.xlim(1, 8)
plt.savefig(os.path.join(OUTPUT_DIR, "correctness_vs_threads_variations.png"))
plt.show()

plt.figure(figsize=(8,6))
for variation in grouped["model_variation"].unique():
    subset = grouped[grouped["model_variation"] == variation]
    plt.plot(subset["num_threads"], subset["cpu_diff"], marker='o', linestyle='-', label=variation)
plt.xlabel("Number of Threads")
plt.ylabel("Average CPU Usage Difference (%)")
plt.title("Average CPU Usage Difference vs Number of Threads\nfor Different Model Variations")
plt.legend(title="Model Variation")
plt.grid(True)
plt.xticks([1, 2, 4, 8])
plt.xlim(1, 8)
plt.savefig(os.path.join(OUTPUT_DIR, "cpu_diff_vs_threads_variations.png"))
plt.show()

plt.figure(figsize=(8,6))
for variation in grouped["model_variation"].unique():
    subset = grouped[grouped["model_variation"] == variation]
    plt.plot(subset["num_threads"], subset["mem_diff"], marker='o', linestyle='-', label=variation)
plt.xlabel("Number of Threads")
plt.ylabel("Average Memory Usage Difference (bytes)")
plt.title("Average Memory Usage Difference vs Number of Threads\nfor Different Model Variations")
plt.legend(title="Model Variation")
plt.grid(True)
plt.xticks([1, 2, 4, 8])
plt.xlim(1, 8)
plt.savefig(os.path.join(OUTPUT_DIR, "mem_diff_vs_threads_variations.png"))
plt.show()

plt.figure(figsize=(8,6))
variations = df["model_variation"].unique()
data_to_plot = [df[df["model_variation"] == var]["elapsed_time_s"] for var in variations]
plt.boxplot(data_to_plot, labels=variations)
plt.xlabel("Model Variation")
plt.ylabel("Inference Time (s)")
plt.title("Inference Time Distribution for Each Model Variation")
plt.grid(True)
plt.savefig(os.path.join(OUTPUT_DIR, "inference_time_boxplot_variations.png"))
plt.show()

print("Plots saved to:", OUTPUT_DIR)

print(f"Loading model '{DEFAULT_MODEL_NAME}' once...")
tokenizer = AutoTokenizer.from_pretrained(DEFAULT_MODEL_NAME, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(DEFAULT_MODEL_NAME, trust_remote_code=True)
model.eval()

pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    torch_dtype=torch.bfloat16,
    device_map="cpu"
)
print("Model loaded successfully.\n")

print(f"Experiment Mode (hard-coded): {EXPERIMENT_MODE}")
all_results = run_inference_experiment(
    pipe=pipe,
    math_dataset=math_dataset,
    experiment_mode=EXPERIMENT_MODE,
    thread_list=THREAD_CONFIGS,
    ram_list=RAM_CONFIGS
)

output_path_json = os.path.join(OUTPUT_DIR, OUTPUT_FILE)
with open(output_path_json, "w", encoding="utf-8") as f:
    json.dump(all_results, f, indent=2)
print(f"All experiments finished. JSON results saved to {output_path_json}")

if OUTPUT_FILE.endswith(".json"):
    csv_file = OUTPUT_FILE.replace(".json", ".csv")
else:
    csv_file = OUTPUT_FILE + ".csv"
output_path_csv = os.path.join(OUTPUT_DIR, csv_file)

csv_headers = list(all_results[0].keys()) if all_results else []
with open(output_path_csv, mode="w", newline="", encoding="utf-8") as csv_f:
    writer = csv.DictWriter(csv_f, fieldnames=csv_headers)
    writer.writeheader()
    for stat in all_results:
        writer.writerow(stat)
print(f"CSV results also saved to {output_path_csv}")

import pandas as pd
print("Plotting results ...")
df = pd.read_csv(output_path_csv)

print(df.head())

if "model_name" in df.columns:
    df = df.drop(columns=["model_name", "input_text",	"reference_solution"	,"generated_text"])

df["num_threads"] = pd.to_numeric(df["num_threads"], errors='coerce')

grouped = df.groupby("num_threads", as_index=False).mean()
print("Grouped DataFrame:")
print(grouped)

plt.figure(figsize=(8,6))
plt.plot(grouped["num_threads"], grouped["elapsed_time_s"], marker='o', linestyle='-')
plt.xlabel("Number of Threads")
plt.ylabel("Average Elapsed Time (s)")
plt.title("Average Inference Time vs Number of Threads")
plt.grid(True)
plt.xticks([1, 2, 4, 8])
plt.xlim(1, 8)

plt.show()

print("Plots saved to:", OUTPUT_DIR)

df = pd.read_csv(output_path_csv)
print("Loaded DataFrame:")
print(df.head())
if "model_name" in df.columns:
    df = df.drop(columns=["model_name", "input_text",	"reference_solution"	,"generated_text"])

if 'num_threads' not in df.columns:
    raise ValueError("CSV中没有 'num_threads' 列，请检查实验数据。")

df["num_threads"] = pd.to_numeric(df["num_threads"], errors='coerce')

grouped = df.groupby("num_threads", as_index=False).mean()
print("Grouped DataFrame:")
print(grouped)

plt.figure(figsize=(8,6))
plt.plot(grouped["num_threads"], grouped["cpu_usage_before"], marker='o', linestyle='-', label="CPU Usage Before")
plt.plot(grouped["num_threads"], grouped["cpu_usage_after"], marker='x', linestyle='-', label="CPU Usage After")
plt.xlabel("Number of Threads")
plt.ylabel("CPU Usage (%)")
plt.title("Average CPU Usage vs Number of Threads")
plt.legend()
plt.grid(True)
plt.show()

plt.figure(figsize=(8,6))
plt.plot(grouped["num_threads"], grouped["mem_usage_before"], marker='o', linestyle='-', label="Memory Usage Before")
plt.plot(grouped["num_threads"], grouped["mem_usage_after"], marker='x', linestyle='-', label="Memory Usage After")
plt.xlabel("Number of Threads")
plt.ylabel("Memory Usage (bytes)")
plt.title("Average Memory Usage vs Number of Threads")
plt.legend()
plt.grid(True)
plt.show()

